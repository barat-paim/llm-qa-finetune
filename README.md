# **Fine-Tuning LLaMA for Question Answering (SQuAD Dataset)**  

## **Overview**  
The code is a fine-tuning pipeline for **LLMs on the SQuAD dataset**, optimized for **question-answering tasks** using:  
- **LoRA (Low-Rank Adaptation)** for efficient fine-tuning  
- **8-bit quantization** to reduce GPU memory usage  
- **Custom GPU memory tracking & early stopping mechanisms**  
- **Evaluation with F1 and Exact Match scores**  

This project explores **the challenges of fine-tuning LLaMA-1B** on resource-constrained hardware and demonstrates **scalable Q&A training techniques.**  

---

## **ğŸš€ Key Features**  
âœ… **Fine-tunes LLaMA-1B** on SQuAD v2 for QA tasks  
âœ… **LoRA optimization** for lightweight training  
âœ… **8-bit quantization** for efficient GPU memory usage  
âœ… **Custom real-time monitoring** of training speed & memory  
âœ… **Evaluation with F1 and EM (Exact Match) scoring**  
âœ… **AWS GPU scaling (T4 x4) with gradient accumulation**  

---

## **ğŸ“ Project Structure**  

```bash
â”œâ”€â”€ llm_qa_finetune/
â”‚   â”œâ”€â”€ finetune.py               # Fine-tuning script (LoRA + Quantization)
â”‚   â”œâ”€â”€ llama_squad_finetune.py   # End-to-end training pipeline
â”‚   â”œâ”€â”€ model_setup.py            # Loads & configures LLaMA model
â”‚   â”œâ”€â”€ process_data.py           # Prepares SQuAD dataset
â”‚   â”œâ”€â”€ eval_2.py                 # Evaluates model using F1 & EM scores
â”‚   â”œâ”€â”€ run_inference.py          # Runs inference on new questions
â”‚   â”œâ”€â”€ test_model.py             # Tests the fine-tuned model with real data
â”‚   â”œâ”€â”€ test_model_loading.py     # Debugging script to check model loading
â”‚   â”œâ”€â”€ README.md                 # Project documentation (this file)
```

---

## **ğŸ› ï¸ Setup & Training**  

### **1ï¸âƒ£ Install Dependencies**  
```bash
pip install -r requirements.txt
```

### **2ï¸âƒ£ Prepare SQuAD Dataset**  
Ensure **SQuAD v2** dataset is downloaded and preprocessed.  
```bash
python process_data.py
```

### **3ï¸âƒ£ Run Fine-Tuning**  
```bash
python llama_squad_finetune.py
```

ğŸ’¡ **To train on AWS GPU (T4 x4):**  
- Update `config.py` with the correct **batch size & learning rate** for larger models.  
- Ensure AWS permissions allow multiple GPUs.  

---

## **ğŸ“Š Evaluation & Results**  

| Model  | Dataset  | F1 Score | EM Score |  
|--------|---------|---------|----------|  
| LLaMA-1B | SQuAD v2 | **82%** | **-** |  

âœ”ï¸ **Final Performance:** Achieved **82% F1** on SQuAD using **LoRA fine-tuning**  
âœ”ï¸ **Challenges:** Could not scale to **LLaMA-7B due to GPU limitations**  
âœ”ï¸ **Solution:** Used **gradient accumulation, quantization, & early stopping**  
âœ”ï¸ **Custom Monitoring:** Built real-time tracking for memory & training speed before switching to **Weights & Biases**  

---

## **ğŸ” Inference (Ask Questions)**  

```bash
python run_inference.py
```
Example:
```python
input_prompt = "What is the capital of France?"
result = run_inference(model_path="./llm_qa_finetune/fine_tuned_llama_squad", tokenizer_path="./llm_qa_finetune/fine_tuned_llama_squad", input_prompt)
print(result)
```

---

## **ğŸ”® Next Steps**  
ğŸ”¹ **Try LLaMA-7B & 12B** with optimized **dataset size & batch tuning**  
ğŸ”¹ **Improve context retention** using better **prompt structuring**  
ğŸ”¹ **Deploy model as an API for real-time Q&A applications**  

---

## **ğŸ” Additional Resources**  

- [LLaMA-1B Model Card](https://huggingface.co/meta-llama/Llama-3.1-1B-Instruct)  
- [SQuAD v2.0 Dataset](https://huggingface.co/datasets/SQuAD)  

---